{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "  Downloading dask-2024.12.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting click>=8.1 (from dask)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpickle>=3.0.0 (from dask)\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rafey\\anaconda3\\envs\\nlp_assn1\\lib\\site-packages (from dask) (24.2)\n",
      "Collecting partd>=1.4.0 (from dask)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pyyaml>=5.3.1 (from dask)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting toolz>=0.10.0 (from dask)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rafey\\anaconda3\\envs\\nlp_assn1\\lib\\site-packages (from click>=8.1->dask) (0.4.6)\n",
      "Collecting locket (from partd>=1.4.0->dask)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Downloading dask-2024.12.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 13.9 MB/s eta 0:00:00\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: toolz, pyyaml, locket, fsspec, cloudpickle, click, partd, dask\n",
      "Successfully installed click-8.1.7 cloudpickle-3.1.0 dask-2024.12.0 fsspec-2024.10.0 locket-1.0.0 partd-1.4.2 pyyaml-6.0.2 toolz-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\Books.json.gz'\n",
    "\n",
    "chunk_size = 5000  # Number of lines per chunk\n",
    "chunks = []\n",
    "\n",
    "# Open and read the .gz file\n",
    "with gzip.open(file_path, 'rt') as f:\n",
    "    chunk = []\n",
    "    for i, line in enumerate(f):\n",
    "        chunk.append(json.loads(line))  # Parse each JSON object\n",
    "        if (i + 1) % chunk_size == 0:\n",
    "            chunks.append(pd.DataFrame(chunk))  # Convert chunk to DataFrame\n",
    "            chunk = []  # Reset chunk\n",
    "\n",
    "    # Process any remaining lines\n",
    "    if chunk:\n",
    "        chunks.append(pd.DataFrame(chunk))\n",
    "\n",
    "# Combine all chunks into a single DataFrame\n",
    "b_df = pd.concat(chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "file_path = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\Books.json.gz'\n",
    "chunk_size = 10000  # Reduce the chunk size\n",
    "chunk_number = 0\n",
    "\n",
    "# Open and read the .gz file\n",
    "with gzip.open(file_path, 'rt') as f:\n",
    "    chunk = []\n",
    "    for i, line in enumerate(f):\n",
    "        chunk.append(json.loads(line))  # Parse each JSON object\n",
    "        if (i + 1) % chunk_size == 0:\n",
    "            chunk_df = pd.DataFrame(chunk)\n",
    "            chunk_df.to_csv(f'D:\\\\A_Raf\\\\MS\\\\research\\\\Bahar_GNN_research\\\\book_data\\\\chunk_{chunk_number}.csv', index=False)\n",
    "            chunk = []  # Reset chunk\n",
    "            chunk_number += 1\n",
    "\n",
    "    # Process any remaining lines\n",
    "    if chunk:\n",
    "        chunk_df = pd.DataFrame(chunk)\n",
    "        chunk_df.to_csv(f'D:\\\\A_Raf\\\\MS\\\\research\\\\Bahar_GNN_research\\\\book_data\\\\chunk_{chunk_number}.csv', index=False)\n",
    "\n",
    "# After processing, you can read the chunks from disk when needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Paths\n",
    "input_folder = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\book_data'  # Replace with the folder containing your CSV files\n",
    "output_folder = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\book_data_filtered'  # Replace with the folder to save filtered files\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "csv_files = glob(os.path.join(input_folder, \"*.csv\"))\n",
    "\n",
    "# Process each file\n",
    "for file_path in csv_files:\n",
    "    # Load the entire CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the 'overall' column exists\n",
    "    if 'overall' in df.columns:\n",
    "        # Filter rows where 'overall' == 5.0\n",
    "        filtered_df = df[df['overall'] == 5.0]\n",
    "        \n",
    "        # Save to output folder with the same filename\n",
    "        output_file_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "        filtered_df.to_csv(output_file_path, index=False)\n",
    "        \n",
    "    #     print(f\"Filtered and saved: {os.path.basename(file_path)}\")\n",
    "    # else:\n",
    "    #     print(f\"Skipped (no 'overall' column): {os.path.basename(file_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "# File path\n",
    "file_path = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\meta_Books.json.gz'\n",
    "\n",
    "# Open and read the .gz file, then load all lines\n",
    "with gzip.open(file_path, 'rt') as f:\n",
    "    data = [json.loads(line) for line in f]  # Parse all JSON objects at once\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "b_meta_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(b_meta_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\meta_book_data.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\meta_book_data.csv'  # Ensure a valid file name\n",
    "b_meta_df.to_csv(file_path, index=False)  # Save the DataFrame as CSV\n",
    "print(f\"DataFrame saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000092878'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_meta_df[\"asin\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Files with selected columns saved in the output folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "source_folder = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\book_data_filtered'  # Folder with chunk CSVs\n",
    "output_folder = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\Book_data_selected_columns'  # Output folder\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['asin', 'reviewTime', 'overall', 'reviewText', 'reviewerID']  # Example columns, update as needed\n",
    "\n",
    "# Loop through all CSV files in the source folder\n",
    "for entry in os.scandir(source_folder):\n",
    "    if entry.is_file() and entry.name.endswith('.csv'):\n",
    "        # Full path of the current chunk file\n",
    "        chunk_file_path = entry.path\n",
    "        \n",
    "        # Load the chunk DataFrame\n",
    "        chunk_df = pd.read_csv(chunk_file_path)\n",
    "        \n",
    "        # Select only the columns you want to keep\n",
    "        chunk_df_selected = chunk_df[columns_to_keep]\n",
    "        \n",
    "        # Save the selected columns to the output folder\n",
    "        output_path = os.path.join(output_folder, entry.name)\n",
    "        chunk_df_selected.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Processing complete. Files with selected columns saved in the output folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafey\\AppData\\Local\\Temp\\ipykernel_6532\\922031576.py:13: DtypeWarning: Columns (1,3,12,13,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reference_df = pd.read_csv(reference_file)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on int64 and object columns for key 'asin'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m chunk_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(chunk_file_path)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Merge with reference data on 'asin'\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 'inner' keeps only matching rows\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Save the merged data only if there are matching rows\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m merged_df\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[1;32mc:\\Users\\rafey\\anaconda3\\envs\\NLP_assn1\\Lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rafey\\anaconda3\\envs\\NLP_assn1\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\rafey\\anaconda3\\envs\\NLP_assn1\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rafey\\anaconda3\\envs\\NLP_assn1\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1508\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1502\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m   1504\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[0;32m   1505\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1506\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[0;32m   1507\u001b[0m     ):\n\u001b[1;32m-> 1508\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on int64 and object columns for key 'asin'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "source_folder = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\book_data_filtered'  # Folder with chunk CSVs\n",
    "reference_file = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\meta_book_data.csv'  # Reference CSV\n",
    "output_folder = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\Book_data_asin'  # Output folder\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load the reference file\n",
    "reference_df = pd.read_csv(reference_file)\n",
    "\n",
    "# Loop through all chunk files in the source folder\n",
    "for entry in os.scandir(source_folder):\n",
    "    if entry.is_file() and entry.name.endswith('.csv') and entry.name.startswith('chunk_'):\n",
    "        # Full path of the current chunk file\n",
    "        chunk_file_path = entry.path\n",
    "        \n",
    "        # Load the chunk DataFrame\n",
    "        chunk_df = pd.read_csv(chunk_file_path)\n",
    "        \n",
    "        # Merge with reference data on 'asin'\n",
    "        merged_df = chunk_df.merge(reference_df, on='asin', how='inner')  # 'inner' keeps only matching rows\n",
    "        \n",
    "        # Save the merged data only if there are matching rows\n",
    "        if not merged_df.empty:\n",
    "            output_path = os.path.join(output_folder, entry.name)\n",
    "            merged_df.to_csv(output_path, index=False)\n",
    "            # print(f\"Saved merged rows to: {output_path}\")\n",
    "\n",
    "print(\"Processing complete. Merged files saved in the output folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r'D:\\A_Raf\\MS\\research\\Bahar_GNN_research\\meta_Books.json.gz'\n",
    "\n",
    "# Initialize an empty list to hold the first 20 items\n",
    "first_20_items = []\n",
    "\n",
    "# Open and read the .gz file\n",
    "with gzip.open(file_path, 'rt') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # Parse each JSON object and append it to the list\n",
    "        first_20_items.append(json.loads(line))\n",
    "        if i == 19:  # Stop after 20 items\n",
    "            break\n",
    "\n",
    "# Convert the list of first 20 items to a DataFrame\n",
    "df_first_20 = pd.DataFrame(first_20_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>tech1</th>\n",
       "      <th>description</th>\n",
       "      <th>fit</th>\n",
       "      <th>title</th>\n",
       "      <th>also_buy</th>\n",
       "      <th>tech2</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature</th>\n",
       "      <th>rank</th>\n",
       "      <th>also_view</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>similar_item</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>asin</th>\n",
       "      <th>imageURL</th>\n",
       "      <th>imageURLHighRes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[It is a biology book with God&amp;apos;s perspect...</td>\n",
       "      <td></td>\n",
       "      <td>Biology Gods Living Creation Third Edition 10 ...</td>\n",
       "      <td>[0669009075, B000K2P5SA, B00MD4G2N0, B000ASIPT...</td>\n",
       "      <td></td>\n",
       "      <td>Keith Graham</td>\n",
       "      <td>[]</td>\n",
       "      <td>1,349,781 in Books (</td>\n",
       "      <td>[0019777701, B000AUCX7I, B000K2P5SA, B001CK63X...</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$39.94</td>\n",
       "      <td>0000092878</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Books, New, Used &amp; Rental Textbooks, Medicine...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Mksap 16 Audio Companion: Medical Knowledge Se...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Acp</td>\n",
       "      <td>[]</td>\n",
       "      <td>1,702,625 in Books (</td>\n",
       "      <td>[B01MUCYEV7, B01KUGTY6O]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>000047715X</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Books, Arts &amp; Photography, Music]</td>\n",
       "      <td></td>\n",
       "      <td>[Discography of American Punk, Hardcore, and P...</td>\n",
       "      <td></td>\n",
       "      <td>Flex! Discography of North American Punk, Hard...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Burkhard Jarisch</td>\n",
       "      <td>[]</td>\n",
       "      <td>6,291,012 in Books (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$199.99</td>\n",
       "      <td>0000004545</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Books, Arts &amp; Photography, Music]</td>\n",
       "      <td></td>\n",
       "      <td>[This is a collection of classic gospel hymns ...</td>\n",
       "      <td></td>\n",
       "      <td>Heavenly Highway Hymns: Shaped-Note Hymnal</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Stamps/Baxter</td>\n",
       "      <td>[]</td>\n",
       "      <td>2,384,057 in Books (</td>\n",
       "      <td>[0006180116, 0996092730, B000QFOGY0, B06WWKNDL...</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0000013765</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Georgina Goodman Nelson Womens Size 8.5 Purple...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>11,735,726 in Books (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$164.10</td>\n",
       "      <td>0000000116</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Books, New, Used &amp; Rental Textbooks, Medicine...</td>\n",
       "      <td></td>\n",
       "      <td>[Brand new; never used.]</td>\n",
       "      <td></td>\n",
       "      <td>Principles of Analgesic Use in the Treatment o...</td>\n",
       "      <td>[0323056962, 0123979285]</td>\n",
       "      <td></td>\n",
       "      <td>American Pain Society</td>\n",
       "      <td>[]</td>\n",
       "      <td>2,906,939 in Books (</td>\n",
       "      <td>[0323056962, 0521879272]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0000555010</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Books, Medical Books, Medicine]</td>\n",
       "      <td></td>\n",
       "      <td>[Flash cards used with accompany MKSAP 15 audi...</td>\n",
       "      <td></td>\n",
       "      <td>MKSAP 15 Audio Companion</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>ACP</td>\n",
       "      <td>[]</td>\n",
       "      <td>2,236,549 in Books (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0000477141</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Books, New, Used &amp; Rental Textbooks, Business...</td>\n",
       "      <td></td>\n",
       "      <td>[Simple Truths of Service: Inspired by Jonny t...</td>\n",
       "      <td></td>\n",
       "      <td>The Simple Truths of Service: Inspired by John...</td>\n",
       "      <td>[1492630519, 0071819045, 0688123163, 160810640...</td>\n",
       "      <td></td>\n",
       "      <td>Visit Amazon's Ken Blanchard Page</td>\n",
       "      <td>[]</td>\n",
       "      <td>2,566,783 in Books (</td>\n",
       "      <td>[0692842004, 1492630519, 1978489552, 160810640...</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0000230022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Books, Education &amp; Teaching, Schools &amp; Teaching]</td>\n",
       "      <td></td>\n",
       "      <td>[This book will alert, amuse and appall you as...</td>\n",
       "      <td></td>\n",
       "      <td>Double-Speak: From Revenue Enhancement to Term...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>William Lutz</td>\n",
       "      <td>[]</td>\n",
       "      <td>2,505,873 in Books (</td>\n",
       "      <td>[0060171340, 0060161345, 0062734121]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$198.70</td>\n",
       "      <td>0000038504</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>LJ Classique Interchangeable Ladies Gift Set W...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>4,368,310 in Books (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0000001589</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Classic Soul Winner's New Testament Bible</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>17,787,141 in Books (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0001019880</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[K4 Phonics, Reading, Writing &amp; Numbers]</td>\n",
       "      <td></td>\n",
       "      <td>K4 Phonics, Reading, Writing, and Numbers Curr...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>4,546,212 in Books (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0000192635</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Books, New, Used &amp; Rental Textbooks]</td>\n",
       "      <td></td>\n",
       "      <td>[Everything you would want to know about echo ...</td>\n",
       "      <td></td>\n",
       "      <td>Echocardiography From a Sonographer's Perspect...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Susan K. DeWitt</td>\n",
       "      <td>[]</td>\n",
       "      <td>3,592,438 in Books (</td>\n",
       "      <td>[0615768350, 0962644455, 0000441031, 032322758...</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0000441058</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>The New England Historical and Genealogical Re...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>New England Historical Genealogical Society</td>\n",
       "      <td>[]</td>\n",
       "      <td>6,027,118 in Books (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$7.45</td>\n",
       "      <td>0000284785</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[Books, Humor &amp; Entertainment, Radio]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>The Sherlock Holmes Audio Collection</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Sir Arthur Conan Doyle</td>\n",
       "      <td>[]</td>\n",
       "      <td>10,519,124 in Books (</td>\n",
       "      <td>[0312089457, 131902856X, 1319083498, 189051708...</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$6.45</td>\n",
       "      <td>0001048236</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[Books, Engineering &amp; Transportation, Engineer...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>The Way Things Work: An Illustrated Encycloped...</td>\n",
       "      <td>[0671210866, B011SJR0HW, 1114119512, 054482438...</td>\n",
       "      <td></td>\n",
       "      <td>C. van Amerongen</td>\n",
       "      <td>[]</td>\n",
       "      <td>142,129 in Books (</td>\n",
       "      <td>[0544824385, 0671210866, B000MXZ8DO, 111411951...</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0000913154</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[Books, Literature &amp; Fiction, Dramas &amp; Plays]</td>\n",
       "      <td></td>\n",
       "      <td>[William Shakespeare is widely regarded as the...</td>\n",
       "      <td></td>\n",
       "      <td>Love's Labour's Lost: Performed by Derek Jacob...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Visit Amazon's William Shakespeare Page</td>\n",
       "      <td>[]</td>\n",
       "      <td>11,922,808 in Books (</td>\n",
       "      <td>[0743484924, 0743482751, 0743477545, 074348283...</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$20.93</td>\n",
       "      <td>0001050230</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[Books, New, Used &amp; Rental Textbooks, Science ...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Foot Rot of Piper nigrum. L. (Plant Science / ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>CABI</td>\n",
       "      <td>[]</td>\n",
       "      <td>20,992,224 in Books (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0000000868</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[Books, Literature &amp; Fiction, Contemporary]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Microserfs</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Visit Amazon's Douglas Coupland Page</td>\n",
       "      <td>[]</td>\n",
       "      <td>13,244,841 in Books (</td>\n",
       "      <td>[0006548598, 1596911050, 031205436X, 067187434...</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0001052292</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[Books, Literature &amp; Fiction, Dramas &amp; Plays]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Measure for Measure Unabridged</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Visit Amazon's William Shakespeare Page</td>\n",
       "      <td>[]</td>\n",
       "      <td>20,297,892 in Books (</td>\n",
       "      <td>[0743484908, 1903436443, 0451527151, 014313173...</td>\n",
       "      <td>Books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$43.51</td>\n",
       "      <td>0001048775</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             category tech1  \\\n",
       "0                                                  []         \n",
       "1   [Books, New, Used & Rental Textbooks, Medicine...         \n",
       "2                  [Books, Arts & Photography, Music]         \n",
       "3                  [Books, Arts & Photography, Music]         \n",
       "4                                                  []         \n",
       "5   [Books, New, Used & Rental Textbooks, Medicine...         \n",
       "6                    [Books, Medical Books, Medicine]         \n",
       "7   [Books, New, Used & Rental Textbooks, Business...         \n",
       "8   [Books, Education & Teaching, Schools & Teaching]         \n",
       "9                                                  []         \n",
       "10                                                 []         \n",
       "11                                                 []         \n",
       "12              [Books, New, Used & Rental Textbooks]         \n",
       "13                                                 []         \n",
       "14              [Books, Humor & Entertainment, Radio]         \n",
       "15  [Books, Engineering & Transportation, Engineer...         \n",
       "16      [Books, Literature & Fiction, Dramas & Plays]         \n",
       "17  [Books, New, Used & Rental Textbooks, Science ...         \n",
       "18        [Books, Literature & Fiction, Contemporary]         \n",
       "19      [Books, Literature & Fiction, Dramas & Plays]         \n",
       "\n",
       "                                          description fit  \\\n",
       "0   [It is a biology book with God&apos;s perspect...       \n",
       "1                                                  []       \n",
       "2   [Discography of American Punk, Hardcore, and P...       \n",
       "3   [This is a collection of classic gospel hymns ...       \n",
       "4                                                  []       \n",
       "5                            [Brand new; never used.]       \n",
       "6   [Flash cards used with accompany MKSAP 15 audi...       \n",
       "7   [Simple Truths of Service: Inspired by Jonny t...       \n",
       "8   [This book will alert, amuse and appall you as...       \n",
       "9                                                  []       \n",
       "10                                                 []       \n",
       "11           [K4 Phonics, Reading, Writing & Numbers]       \n",
       "12  [Everything you would want to know about echo ...       \n",
       "13                                                 []       \n",
       "14                                                 []       \n",
       "15                                                 []       \n",
       "16  [William Shakespeare is widely regarded as the...       \n",
       "17                                                 []       \n",
       "18                                                 []       \n",
       "19                                                 []       \n",
       "\n",
       "                                                title  \\\n",
       "0   Biology Gods Living Creation Third Edition 10 ...   \n",
       "1   Mksap 16 Audio Companion: Medical Knowledge Se...   \n",
       "2   Flex! Discography of North American Punk, Hard...   \n",
       "3          Heavenly Highway Hymns: Shaped-Note Hymnal   \n",
       "4   Georgina Goodman Nelson Womens Size 8.5 Purple...   \n",
       "5   Principles of Analgesic Use in the Treatment o...   \n",
       "6                            MKSAP 15 Audio Companion   \n",
       "7   The Simple Truths of Service: Inspired by John...   \n",
       "8   Double-Speak: From Revenue Enhancement to Term...   \n",
       "9   LJ Classique Interchangeable Ladies Gift Set W...   \n",
       "10          Classic Soul Winner's New Testament Bible   \n",
       "11  K4 Phonics, Reading, Writing, and Numbers Curr...   \n",
       "12  Echocardiography From a Sonographer's Perspect...   \n",
       "13  The New England Historical and Genealogical Re...   \n",
       "14               The Sherlock Holmes Audio Collection   \n",
       "15  The Way Things Work: An Illustrated Encycloped...   \n",
       "16  Love's Labour's Lost: Performed by Derek Jacob...   \n",
       "17  Foot Rot of Piper nigrum. L. (Plant Science / ...   \n",
       "18                                         Microserfs   \n",
       "19                     Measure for Measure Unabridged   \n",
       "\n",
       "                                             also_buy tech2  \\\n",
       "0   [0669009075, B000K2P5SA, B00MD4G2N0, B000ASIPT...         \n",
       "1                                                  []         \n",
       "2                                                  []         \n",
       "3                                                  []         \n",
       "4                                                  []         \n",
       "5                            [0323056962, 0123979285]         \n",
       "6                                                  []         \n",
       "7   [1492630519, 0071819045, 0688123163, 160810640...         \n",
       "8                                                  []         \n",
       "9                                                  []         \n",
       "10                                                 []         \n",
       "11                                                 []         \n",
       "12                                                 []         \n",
       "13                                                 []         \n",
       "14                                                 []         \n",
       "15  [0671210866, B011SJR0HW, 1114119512, 054482438...         \n",
       "16                                                 []         \n",
       "17                                                 []         \n",
       "18                                                 []         \n",
       "19                                                 []         \n",
       "\n",
       "                                          brand feature  \\\n",
       "0                                  Keith Graham      []   \n",
       "1                                           Acp      []   \n",
       "2                              Burkhard Jarisch      []   \n",
       "3                                 Stamps/Baxter      []   \n",
       "4                                                    []   \n",
       "5                         American Pain Society      []   \n",
       "6                                           ACP      []   \n",
       "7             Visit Amazon's Ken Blanchard Page      []   \n",
       "8                                  William Lutz      []   \n",
       "9                                                    []   \n",
       "10                                                   []   \n",
       "11                                                   []   \n",
       "12                              Susan K. DeWitt      []   \n",
       "13  New England Historical Genealogical Society      []   \n",
       "14                       Sir Arthur Conan Doyle      []   \n",
       "15                             C. van Amerongen      []   \n",
       "16      Visit Amazon's William Shakespeare Page      []   \n",
       "17                                         CABI      []   \n",
       "18         Visit Amazon's Douglas Coupland Page      []   \n",
       "19      Visit Amazon's William Shakespeare Page      []   \n",
       "\n",
       "                     rank                                          also_view  \\\n",
       "0    1,349,781 in Books (  [0019777701, B000AUCX7I, B000K2P5SA, B001CK63X...   \n",
       "1    1,702,625 in Books (                           [B01MUCYEV7, B01KUGTY6O]   \n",
       "2    6,291,012 in Books (                                                 []   \n",
       "3    2,384,057 in Books (  [0006180116, 0996092730, B000QFOGY0, B06WWKNDL...   \n",
       "4   11,735,726 in Books (                                                 []   \n",
       "5    2,906,939 in Books (                           [0323056962, 0521879272]   \n",
       "6    2,236,549 in Books (                                                 []   \n",
       "7    2,566,783 in Books (  [0692842004, 1492630519, 1978489552, 160810640...   \n",
       "8    2,505,873 in Books (               [0060171340, 0060161345, 0062734121]   \n",
       "9    4,368,310 in Books (                                                 []   \n",
       "10  17,787,141 in Books (                                                 []   \n",
       "11   4,546,212 in Books (                                                 []   \n",
       "12   3,592,438 in Books (  [0615768350, 0962644455, 0000441031, 032322758...   \n",
       "13   6,027,118 in Books (                                                 []   \n",
       "14  10,519,124 in Books (  [0312089457, 131902856X, 1319083498, 189051708...   \n",
       "15     142,129 in Books (  [0544824385, 0671210866, B000MXZ8DO, 111411951...   \n",
       "16  11,922,808 in Books (  [0743484924, 0743482751, 0743477545, 074348283...   \n",
       "17  20,992,224 in Books (                                                 []   \n",
       "18  13,244,841 in Books (  [0006548598, 1596911050, 031205436X, 067187434...   \n",
       "19  20,297,892 in Books (  [0743484908, 1903436443, 0451527151, 014313173...   \n",
       "\n",
       "   main_cat similar_item date    price        asin imageURL imageURLHighRes  \n",
       "0     Books                     $39.94  0000092878       []              []  \n",
       "1     Books                             000047715X       []              []  \n",
       "2     Books                    $199.99  0000004545       []              []  \n",
       "3     Books                             0000013765       []              []  \n",
       "4     Books                    $164.10  0000000116       []              []  \n",
       "5     Books                             0000555010       []              []  \n",
       "6     Books                             0000477141       []              []  \n",
       "7     Books                             0000230022       []              []  \n",
       "8     Books                    $198.70  0000038504       []              []  \n",
       "9     Books                             0000001589       []              []  \n",
       "10    Books                             0001019880       []              []  \n",
       "11    Books                             0000192635       []              []  \n",
       "12    Books                             0000441058       []              []  \n",
       "13    Books                      $7.45  0000284785       []              []  \n",
       "14    Books                      $6.45  0001048236       []              []  \n",
       "15    Books                             0000913154       []              []  \n",
       "16    Books                     $20.93  0001050230       []              []  \n",
       "17    Books                             0000000868       []              []  \n",
       "18    Books                             0001052292       []              []  \n",
       "19    Books                     $43.51  0001048775       []              []  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_first_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"overall\": 5.0, \"verified\": true, \"reviewTime\": \"03 11, 2013\", \"reviewerID\": \"A3478QRKQDOPQ2\", \"asin\": \"0001527665\", \"style\": {\"Format:\": \" VHS Tape\"}, \"reviewerName\": \"jacki\", \"reviewText\": \"really happy they got evangelised .. spoiler alert==happy ending liked that..since started bit worrisome... but yeah great stories these missionary movies, really short only half hour but still great\", \"summary\": \"great\", \"unixReviewTime\": 1362960000}\n",
      "{\"overall\": 5.0, \"vote\": \"3\", \"verified\": true, \"reviewTime\": \"02 18, 2013\", \"reviewerID\": \"A2VHSG6TZHU1OB\", \"asin\": \"0001527665\", \"style\": {\"Format:\": \" Amazon Video\"}, \"reviewerName\": \"Ken P\", \"reviewText\": \"Having lived in West New Guinea (Papua) during the time period covered in this video, it is realistic, accurate, and conveys well the entrance of light and truth into a culture that was for centuries dead to and alienated from God.\", \"summary\": \"Realistic and Accurate\", \"unixReviewTime\": 1361145600}\n",
      "{\"overall\": 5.0, \"verified\": false, \"reviewTime\": \"01 17, 2013\", \"reviewerID\": \"A23EJWOW1TLENE\", \"asin\": \"0001527665\", \"style\": {\"Format:\": \" Amazon Video\"}, \"reviewerName\": \"Reina Berumen\", \"reviewText\": \"Excellent look into contextualizing the Gospel and God's sovereignty over cultural barriers. The book and movie are both captivating. I would definitely recommend to both Christians and non-believers.\", \"summary\": \"Peace Child\", \"unixReviewTime\": 1358380800}\n",
      "{\"overall\": 5.0, \"verified\": true, \"reviewTime\": \"01 10, 2013\", \"reviewerID\": \"A1KM9FNEJ8Q171\", \"asin\": \"0001527665\", \"style\": {\"Format:\": \" Amazon Video\"}, \"reviewerName\": \"N Coyle\", \"reviewText\": \"More than anything, I've been challenged to find ways to share Christ is a culturally relevant way to those around me.  Peace child is a cherished \\\"how to\\\" for me to do that.\", \"summary\": \"Culturally relevant ways to share the love of Christ is a message for us all.\", \"unixReviewTime\": 1357776000}\n",
      "{\"overall\": 4.0, \"verified\": true, \"reviewTime\": \"12 26, 2012\", \"reviewerID\": \"A38LY2SSHVHRYB\", \"asin\": \"0001527665\", \"style\": {\"Format:\": \" Amazon Video\"}, \"reviewerName\": \"Jodie Vesely\", \"reviewText\": \"This is a great movie for a missionary going into a foreign country, especially one that is not used to foreign presence. But, it was a little on the short side.\", \"summary\": \"Good Movie! Great for cross-cultural missionaries!\", \"unixReviewTime\": 1356480000}\n",
      "{\"overall\": 5.0, \"vote\": \"3\", \"verified\": true, \"reviewTime\": \"11 16, 2012\", \"reviewerID\": \"AHTYUW2H1276L\", \"asin\": \"0001527665\", \"style\": {\"Format:\": \" Amazon Video\"}, \"reviewerName\": \"lilsistah21\", \"reviewText\": \"This movie was in ENGLISH....it was a great summary of the book and the experience of the Richardsons while in New Guinea.\", \"summary\": \"Great....\", \"unixReviewTime\": 1353024000}\n",
      "{\"overall\": 5.0, \"vote\": \"4\", \"verified\": true, \"reviewTime\": \"07 15, 2012\", \"reviewerID\": \"A3M3HCZLXW0YLF\", \"asin\": \"0001527665\", \"style\": {\"Format:\": \" Amazon Video\"}, \"reviewerName\": \"Carla\", \"reviewText\": \"This is a fascinating true story, well acted by many of the actual characters.  Watch this and see just one of countless examples of how faithfully God works out ways for all of mankind to find Him, in every culture.\", \"summary\": \"A remarkable true story, told in English (contrary to reports from earlier reviews)\", \"unixReviewTime\": 1342310400}\n",
      "{\"overall\": 1.0, \"verified\": true, \"reviewTime\": \"09 3, 2010\", \"reviewerID\": \"A1OMHX76O2NC6V\", \"asin\": \"0001527665\", \"style\": {\"Format:\": \" Amazon Video\"}, \"reviewerName\": \"Richard Ellis\", \"reviewText\": \"This DVD appears to be in German. It is not in english. I do not speak German.  This needs to be corrected. I want my $1.99 back.\", \"summary\": \"Peace Child DVD\", \"unixReviewTime\": 1283472000}\n",
      "{\"overall\": 1.0, \"verified\": true, \"reviewTime\": \"05 7, 2010\", \"reviewerID\": \"A3OBOZ41IK6O1M\", \"asin\": \"0001527665\", \"style\": {\"Format:\": \" Amazon Video\"}, \"reviewerName\": \"Adam\", \"reviewText\": \"This movie is not in English although the title of the movie is as is the book. There is no forewarning anywhere that the movie is not in English. Sounds like it might be Russian?\", \"summary\": \"Not in English\", \"unixReviewTime\": 1273190400}\n",
      "{\"overall\": 5.0, \"verified\": true, \"reviewTime\": \"11 9, 2012\", \"reviewerID\": \"A2M1CU2IRZG0K9\", \"asin\": \"0005089549\", \"style\": {\"Format:\": \" VHS Tape\"}, \"reviewerName\": \"Terri\", \"reviewText\": \"So sorry I didn't purchase this years ago when it first came out!!  This is very good and entertaining!  We absolutely loved it and anticipate seeing it repeatedly.  We actually wore out the cassette years back, so we also purchased this same product on cd.  Best purchase we made out of all!  Would purchase on dvd if we could find one.\", \"summary\": \"Amazing!\", \"unixReviewTime\": 1352419200}\n",
      "{\"overall\": 5.0, \"verified\": true, \"reviewTime\": \"09 11, 2012\", \"reviewerID\": \"A1XIXLXK9B4DAJ\", \"asin\": \"0005089549\", \"style\": {\"Format:\": \" Audio CD\"}, \"reviewerName\": \"MMnMM\", \"reviewText\": \"Product received quickly from seller. Product was in gr\n"
     ]
    }
   ],
   "source": [
    "print(user_movie_data[0:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_user_data_lines = user_movie_data.strip().split('\\n')\n",
    "m_user_data_dicts = [json.loads(line) for line in m_user_data_lines]\n",
    "m_user_df = pd.DataFrame(m_user_data_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_assn1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
