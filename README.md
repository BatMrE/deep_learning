Learning rate = 0.0001	Learning rate = 0.001	Learning rate = 0.01	Learning rate = 0.05
Adaline BGD	Training Loss = 6.659


Val. loss = 9.315



Test Loss = 9.416	Training Loss = 11046.513


Val. loss = 1656212.36


Test Loss = 2168786.57	Training Loss = 5373997630002.89


Val. loss= 9.162615815613731e+16


TEST LOSS = 1.127060963762328e+17	Training Loss = 2.41933952826602e+18


Val. loss= 1.0426907652988485e+24


TEST LOSS = 10.509340271873054
Adaline SGD	Training Loss = 6.264

Val.loss = 9.522

Test Loss = 9.664	Training Loss = 4.7612671815944125

Val. loss= 9.149202585697784

TEST LOSS = 9.16894512502506	Training Loss = 4.675


Val. loss= 9.33


TEST LOSS = 9.267	Training Loss = 72.044


Val. loss= 9.82


TEST LOSS = 10.509
Sigmoid BGD	Training Loss = 10.059

Val.loss = 19.294


Test Loss = 21.408	Training Loss = 9.579


Val. loss= 19.162


TEST LOSS = 21.217	Training Loss = 4.7612671815944125

Val. loss= 9.149202585697784

TEST LOSS = 9.16894512502506	Training Loss = 9.539


Val. loss= 19.153


TEST LOSS = 21.217
Sigmoid SGD	Training Loss = 9.903

Val. loss = 19.309

Test Loss = 21.429	Training Loss = 9.579


Val. loss= 19.162

TEST LOSS = 21.217	Training Loss = 9.78


Val. loss= 19.13

TEST LOSS = 21.968	Training Loss = 9.53


Val. loss= 19.15

TEST LOSS = 21.217




Perceptron
	Learning rate 0.001	Learning rate 0.01	Learning rate 0.05
Average Validation Loss	0.58	0.060	0.101
Average Validation Loss	0.967	0.982	0.976



Pocket Algorithm
	Learning rate 0.001	Learning rate 0.01	Learning rate 0.05
Average Validation Loss	0.209	0.076	0.111
Average Validation Loss	0.965	0.982	0.984

